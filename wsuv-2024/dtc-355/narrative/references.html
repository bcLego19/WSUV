<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Multimodal Narrative (NLP)</title>
	<link rel="stylesheet" type="text/css" href="styles/style.css">
</head>
<body>

	<div id="wrapper">

		<header>

			<div id="navbar">

				<ul>
					<li><a href="index.html">Home</a></li>
					<li><a class="active" href="references.html">References</a>
					</li>
					<li><a href="about.html">About</a></li>
				</ul>

			</div>

		</header>

		<h1>For Further Reading</h1>

		<div id="references-content">

			<div id="references-summary">

				<h3>Text References</h3>

				<ul>
					<li>
						Jurafsky, D., & Martin, J. H. (2021). Speech and Language Processing (3rd ed.). Pearson Education Limited.
					</li>
					<li>
						Mitchell, T. M. (1997). Machine Learning. McGraw-Hill.
					</li>
					<li>
						NLTK Book: Natural Language Processing with Python <a href="https://www.nltk.org/book/ch01.html">[>>]</a>
					</li>
					<li>
						Wikipedia: Natural Language Processing <a href="https://en.wikipedia.org/wiki/Natural_language_processing">[>>]</a>
					</li>
					<li>
						Hodges, A. (1983). Alan Turing: The Enigma. Vintage Books.
					</li>
					<li>
						Anderson, K. (1997). Turing's Cathedral: Origins of the Digital Universe. Bloomsbury.
					</li>
					<li>
						Wikipedia: Alan Turing <a href="https://en.wikipedia.org/wiki/Alan_Turing">[>>]</a>
					</li>
					<li>
						Wikipedia: Georgetown-IBM Experiment <a href="https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment">[>>]</a>
					</li>
					<li>
						McCarthy, J. (1955). A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence.
					</li>
					<li>
						Wikipedia: Dartmouth Summer Research Project on Artificial Intelligence <a href="https://en.wikipedia.org/wiki/Dartmouth_workshop">[>>]</a>
					</li>
					<li>
						Wikipedia: Machine Translation <a href="https://en.wikipedia.org/wiki/Machine_translation">[>>]</a>
					</li>
					<li>
						Wikipedia: Syntactic Parsing (computational linguistics) <a href="https://en.wikipedia.org/wiki/Syntactic_parsing_(computational_linguistics)">[>>]</a>
					</li>
					<li>
						Shortliffe, E. H. (1976). Computer-Based Medical Consultation: MYCIN. Elsevier.
					</li>
					<li>
						Weizenbaum, J. (1966). ELIZA: A computer program for the study of natural language communication between man and machine.
					</li>
					<li>
						Biber, D., Conrad, S., & Leech, G. (2006). Longman Student Grammar of Spoken and Written English.
					</li>
					<li>
						McEnery, T., Wilson, A., & Xiao, R. (2006). Corpus-Based Language Studies.
					</li>
					<li>
						Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning.
					</li>
					<li>
						Wikipedia: Deep Learning <a href="https://en.wikipedia.org/wiki/Deep_learning">[>>]</a>
					</li>
					<li>
						Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Efficient estimation of word representations in vector space.
					</li>
					<li>
						Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation.
					</li>
					<li>
						Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention Is All You Need.
					</li>
					<li>
						Wikipedia: Transformer (deep learning architecture) <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)">[>>]</a>
					</li>
					<li>
						Brown, T. B., Mann, B., Ryder, N., Subbiah, J., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners.
					</li>
					<li>
						OpenAI: GPT-3 <a href="https://openai.com/index/gpt-3-apps/">
					  [>>]
					</a>
					</li>
					<li>
						Wikipedia: GPT-3 <a href="https://en.wikipedia.org/wiki/GPT-3">
					  [>>]
					</a>
					</li>
					<li>
						Google AI: LaMDA <a href="https://blog.google/technology/ai/lamda/">
					  [>>]
					</a>
					</li>
					<li>
						Wikipedia: LaMDA <a href="https://en.wikipedia.org/wiki/LaMDA">
					  [>>]
					</a>
					</li>
				</ul>

				<h3>Image References</h3>

				<ul>
					<li>
						First Public Demonstration of Machine Translation Occurs <a href="https://www.historyofinformation.com/detail.php?id=666">[>>]</a>
					</li>
					<li>
						Original Proposal of Dartmouth Project <a href="https://home.dartmouth.edu/about/dartmouth-milestones">[>>]</a>
					</li>
					<li>
						Headline of IBM's Article on ALPS <a href="https://mt-archive.net/70/LangMonthly-39-1986-ALPS.pdf">[>>]</a>
					</li>
					<li>
						A diagram of syntactic parsing <a href="https://en.wikipedia.org/wiki/File:LF_OpenNLP_Parser.jpg">[>>]</a>
					</li>
					<li>
						Formula for calculating mycin's certainty factor <a href="https://en.wikipedia.org/wiki/Mycin">[>>]</a>
					</li>
					<li>
						A model on machine learning performance <a href="https://www.researchgate.net/figure/Machine-learning-model-performance-AUC-area-under-the-curve-RF-random-forest_fig3_373198566">[>>]</a>
					</li>
					<li>
						A pdf from stanford involving Statistical Machine Translation (SMT) <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/handouts/Collins_annotated.pdf">[>>]</a>
					</li>
					<li>
						A simplified representation of a single time step of the abstract numerical scheme <a href="https://www.researchgate.net/publication/328934911_A_machine_learning_framework_for_data_driven_acceleration_of_computations_of_di_erential_equations">[>>]</a>
					</li>
					<li>
						An octopus reading books while fish talk about it. <a href="https://www.deviantart.com/harshadpd/art/Deep-Learning-994225304">[>>]</a>
					</li>
					<li>
						A visualization of word embeddings <a href="https://commons.wikimedia.org/wiki/File:T-SNE_visualisation_of_word_embeddings_generated_using_19th_century_literature.png">[>>]</a>
					</li>
					<li>
						A picture showing the architecture of the transformer model. <a href="https://commons.wikimedia.org/wiki/File:The-Transformer-model-architecture.png">[>>]</a>
					</li>
					<li>
						A figure illustrating the direct use of Large Language Models <a href="https://www.researchgate.net/figure/This-figure-illustrates-the-direct-use-of-Large-Language-Models-LLMs-in-generating_fig1_384189957">[>>]</a>
					</li>
				</ul>

			</div>

		</div>

	</div>

	<footer>
		Conner Batson 2024
	</footer>

</body>
</html>